{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ApproxHessian.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUv2hPtc3wfv42ctjH+yR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maciejskorski/ml_examples/blob/master/ApproxHessian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmZNoo7r6D-r"
      },
      "source": [
        "## Loss Landscape\n",
        "\n",
        "use random orthogonal directions  https://towardsdatascience.com/visualizing-loss-landscape-of-deep-neural-networks-but-can-we-trust-them-3d3ae0cff46e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRMPkdHsX9ds"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "import itertools\n",
        "\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZyGl9fIyakg"
      },
      "source": [
        "## define the LeNet5 architecture\n",
        "\n",
        "class LeNet5(tf.keras.Model):\n",
        "  ''' see https://en.wikipedia.org/wiki/Convolutional_neural_network '''\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LeNet5, self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(6,kernel_size=(5,5), strides=(1, 1), activation='tanh',padding='valid')\n",
        "    self.avpool1 = tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid')\n",
        "    self.conv2 = tf.keras.layers.Conv2D(16, kernel_size=(5,5), strides=(1, 1), activation='tanh', padding='valid')\n",
        "    self.avpool2 = tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid')\n",
        "    self.flatt = tf.keras.layers.Flatten()\n",
        "    self.dense1 = tf.keras.layers.Dense(120, activation='tanh')\n",
        "    self.dense2 = tf.keras.layers.Dense(84, activation='tanh')\n",
        "    self.dense3 = tf.keras.layers.Dense(10, activation='linear')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.avpool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.avpool2(x)\n",
        "    x = self.flatt(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x\n",
        "\n",
        "class LogisticRegression(tf.keras.Model):\n",
        "  ''' baseline model '''\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.flatt = tf.keras.layers.Flatten()\n",
        "    self.dense1 = tf.keras.layers.Dense(100,activation='relu')\n",
        "    self.dense2 = tf.keras.layers.Dense(10,activation='linear')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.flatt(inputs)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gps6L4tx0PA"
      },
      "source": [
        "## prepare training data\n",
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n",
        "x_train,x_test = np.expand_dims(x_train,-1),np.expand_dims(x_test,-1) # when black-white\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "data = data.batch(32).prefetch(1).repeat(1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXlevcPK-iS8"
      },
      "source": [
        "## loss and train\n",
        "\n",
        "model = LeNet5()\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "@tf.function\n",
        "def logp(model,x,y):\n",
        "  ''' model outputs logits for prediction '''\n",
        "  y_pred = model(x)\n",
        "  logp = -tf.keras.losses.sparse_categorical_crossentropy(y,y_pred,from_logits=True)\n",
        "  return tf.reduce_mean(logp)\n",
        "\n",
        "@tf.function\n",
        "def train_step(x,y):\n",
        "  loss_fn = lambda: -logp(model,x,y)\n",
        "  vars_fn = lambda: model.trainable_variables\n",
        "  optimizer.minimize(loss_fn,vars_fn)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb7q2sxQoyWk"
      },
      "source": [
        "## utils\n",
        "\n",
        "def random_orthogonal_pair(N):\n",
        "  ''' generate randomly two orthogonal directions '''\n",
        "  t = np.random.normal(size=(2,N))\n",
        "  t = t/np.linalg.norm(t,axis=1,keepdims=True)\n",
        "  a,b = t\n",
        "  b = b-a*a.dot(b)\n",
        "  b = b/np.linalg.norm(b)\n",
        "  return a,b\n",
        "\n",
        "def model2flatt(model):\n",
        "  ''' model tensors to one flatt tensor '''\n",
        "  return np.concatenate([t.numpy().ravel() for t in model.trainable_variables])\n",
        "\n",
        "def flatt2model(w,shapes):\n",
        "  ''' one flatt tensor to model tensors '''\n",
        "  offsets = [np.prod(s) for s in shapes]\n",
        "  pos_end = list(np.cumsum(offsets))\n",
        "  pos_start = [0]+pos_end[:-1]\n",
        "  return [w[i:j].reshape(s) for (i,j,s) in zip(pos_start,pos_end,shapes)]\n",
        "\n",
        "def loss_on_weights(model,x,y,w):\n",
        "  ''' loss depending on custom choice of weights '''\n",
        "  # keep the original weights\n",
        "  vals_original = [t.numpy() for t in model.trainable_variables]\n",
        "  # change weigts in the model and call it\n",
        "  vals = flatt2model(w,[t.shape.as_list() for t in model.trainable_variables])\n",
        "  for val,var in zip(vals,model.trainable_variables):\n",
        "    var.assign(val)\n",
        "  out = logp(model,x,y)\n",
        "  # restore the original weights\n",
        "  for val,var in zip(vals_original,model.trainable_variables):\n",
        "    var.assign(val)\n",
        "  return out\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmB4sbQEyEjh",
        "outputId": "3c7f82f0-ce8a-4b3b-89b6-6651e4010ff3"
      },
      "source": [
        "## train\n",
        "\n",
        "for i,(x,y) in enumerate(data):\n",
        "  if i%100 == 0:\n",
        "    print(logp(model,x_test,y_test).numpy())\n",
        "  train_step(x,y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.3125286\n",
            "-0.814244\n",
            "-0.6945786\n",
            "-0.6621536\n",
            "-0.614\n",
            "-0.5866022\n",
            "-0.58388\n",
            "-0.56756926\n",
            "-0.54807186\n",
            "-0.5408189\n",
            "-0.51329786\n",
            "-0.51388514\n",
            "-0.51534396\n",
            "-0.50051004\n",
            "-0.51413697\n",
            "-0.4669662\n",
            "-0.46243623\n",
            "-0.4804576\n",
            "-0.46161178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ipVkX9BRgW"
      },
      "source": [
        "## compute the loss on weights along two random orthogonal dimensions!\n",
        "\n",
        "np.random.seed(1234)\n",
        "N = model.count_params()\n",
        "a,b=random_orthogonal_pair(N)\n",
        "w0 = model2flatt(model)\n",
        "\n",
        "def fn(a_step,b_step):\n",
        "  return loss_on_weights(model,x_test[:1000],y_test[:1000],w0+a_step*a+b_step*b)\n",
        "\n",
        "fn = np.vectorize(fn)\n",
        "\n",
        "xs,ys = 10*np.linspace(-1,1,15),10*np.linspace(-1,1,15)\n",
        "grid = np.meshgrid(xs,ys,sparse=True)\n",
        "X,Y,Z = *grid,fn(*grid)\n",
        "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"},figsize=(6,6))\n",
        "ax.set_xlabel('random direction 1')\n",
        "ax.set_ylabel('random direction 2')\n",
        "ax.set_zlabel('loss wrt model params',rotation=90,labelpad=6)\n",
        "surf = ax.plot_surface(X,Y,-Z, cmap=cm.viridis, linewidth=0, antialiased=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWBNH5D-0GuG"
      },
      "source": [
        "(model(x_test).numpy().argmax(1)==y_test).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNZCeUOlcQVU"
      },
      "source": [
        "## Activations near 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWCiOuejcTK4"
      },
      "source": [
        "sn.set_style(\"darkgrid\")\n",
        "palette = itertools.cycle(sn.color_palette())\n",
        "\n",
        "fs = [tf.keras.activations.linear,tf.keras.activations.tanh,tf.keras.activations.sigmoid,tf.keras.activations.relu]\n",
        "#fs = [np.vectorize(f) for f in fs]\n",
        "\n",
        "fig,axs = plt.subplots(1,4,figsize=(12,3))\n",
        "\n",
        "xs = np.linspace(-2,2,100)\n",
        "for (f,ax) in zip(fs,axs.ravel()):\n",
        "  ax.plot(xs,f(xs),label=f.__name__,color=next(palette))\n",
        "  ax.legend()\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdvFffi6Ii15"
      },
      "source": [
        "## Hessian Approximation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FubqldDv0KAI"
      },
      "source": [
        "### utils\n",
        "\n",
        "## utils for exact hessian form\n",
        "\n",
        "@tf.function\n",
        "def logp(logits,y):\n",
        "  ''' logits of shape (n_batch,n_class), y of shape (n_batch,) '''\n",
        "  logp = tf.gather(logits,y,batch_dims=1) - tf.reduce_logsumexp(logits,-1) # (n_batch,)\n",
        "  return tf.reduce_mean(logp) # (,)\n",
        "\n",
        "@tf.function\n",
        "def grad(model,var,x,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x)\n",
        "    loss = -logp(logits,y)\n",
        "  grad = tape.gradient(loss,var)\n",
        "  return grad\n",
        "\n",
        "@tf.function\n",
        "def hvp(model,var,vec,x,y):\n",
        "  ''' hessian-vector product; takes advantage of weighted gradient (hess is with respect to weight matrix) '''\n",
        "  # second derivative (on top)\n",
        "  with tf.GradientTape() as outer_tape:\n",
        "    # first derivative (inner)\n",
        "    with tf.GradientTape() as inner_tape:\n",
        "      logits = model(x)\n",
        "      loss = -logp(logits,y)\n",
        "    grads = inner_tape.gradient(loss,var)\n",
        "  hess_vec = outer_tape.gradient(grads,var,output_gradients=vec)\n",
        "  return hess_vec\n",
        "\n",
        "@tf.function\n",
        "def hform(model,var,vec,x,y):\n",
        "  ''' hessian form of model loss, with respect to variable, evaluated at vector '''\n",
        "  h = hvp(model,var,vec,x,y)\n",
        "  return tf.reduce_sum(vec*h)\n",
        "\n",
        "## utils for approx hessian form\n",
        "\n",
        "@tf.function\n",
        "def jac_logits_var(model,var,x,y=None):\n",
        "  ''' jacobian of logits wrt model variable '''\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x)\n",
        "  jac = tape.jacobian(logits,var)\n",
        "  return jac\n",
        "\n",
        "@tf.function\n",
        "def hess_loss_logits(model,x,y):\n",
        "  ''' hessian of loss wrt logits '''\n",
        "  with tf.GradientTape() as outer_tape:\n",
        "    with tf.GradientTape() as inner_tape:\n",
        "      logits = model(x)\n",
        "      loss=-logp(logits,y) # (n_batch,n_class)\n",
        "    grads = inner_tape.gradient(loss,logits) # (n_batch,n_class)\n",
        "  return outer_tape.batch_jacobian(grads,logits)  # (n_batch,n_class,n_class)\n",
        "\n",
        "@tf.function\n",
        "def hform_approx(model,var,vec,x,y):\n",
        "  ''' hessian form wrt variable, evaluated on vector; approximate '''\n",
        "  H = hess_loss_logits(model,x,y) # (n_batch,n_class,n_class)\n",
        "  J = jac_logits_var(model,var,x) # (n_batch,n_class,var.shape)\n",
        "  vec = grad(model,var,x,y) # (var.shape,)\n",
        "  #J_dot_vec = tf.einsum('abcd,cd->ab',J,vec) # (n_batch,n_class)\n",
        "  axes = [range(2,2+len(vec.shape)),range(len(vec.shape))]\n",
        "  J_dot_vec = tf.tensordot(J,vec,axes=axes)\n",
        "  Hform = tf.einsum('xab,xa,xb->',H,J_dot_vec,J_dot_vec)\n",
        "  return Hform\n",
        "\n",
        "## utils to refresh initialization\n",
        "\n",
        "def reset_kernels(model):\n",
        "  for l in model.layers:\n",
        "    if hasattr(l,'kernel'):\n",
        "      l.kernel.assign(l.kernel_initializer(l.kernel.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wctb84Alt-M-"
      },
      "source": [
        "## define approximation error\n",
        "\n",
        "@tf.function\n",
        "def happrox_error(model,var,x,y):\n",
        "  vec = grad(model,var,x,y) \n",
        "  vec_norm = tf.linalg.norm(vec)\n",
        "  h_exact = hform(model,var,vec,x,y)\n",
        "  h_approx = hform_approx(model,var,vec,x,y)\n",
        "  error = tf.abs(h_approx-h_exact) / tf.square(vec_norm)\n",
        "  return error\n",
        "\n",
        "def reset_kernels(model):\n",
        "  for l in model.layers:\n",
        "    if hasattr(l,'kernel'):\n",
        "      l.kernel.assign(l.kernel_initializer(l.kernel.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm8sOL5akJ4h"
      },
      "source": [
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "sn.set_style(\"darkgrid\")\n",
        "\n",
        "## load and normalize data\n",
        "\n",
        "(x_train,y_train),_ = tf.keras.datasets.cifar10.load_data()\n",
        "#x_train = x_train.reshape(-1,28*28)\n",
        "x_train = x_train / 255\n",
        "x_train = x_train - x_train.mean(0)\n",
        "y_train = y_train.astype('int32')\n",
        "x_train = x_train.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezlAV63zxMTj"
      },
      "source": [
        "## LeNet-5\n",
        "\n",
        "activation = tf.keras.activations.tanh\n",
        "\n",
        "inputs = tf.keras.Input(shape=(32,32,3))\n",
        "\n",
        "conv1 = tf.keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation=activation,name='conv1')(inputs)\n",
        "avpool1 = tf.keras.layers.AveragePooling2D()(conv1)\n",
        "\n",
        "conv2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation=activation,name='conv2')(avpool1)\n",
        "avpool2 = tf.keras.layers.AveragePooling2D()(conv2)\n",
        "\n",
        "flatt = tf.keras.layers.Flatten()(avpool2)\n",
        "dense1 = tf.keras.layers.Dense(units=120, activation=activation,name='dense1')(flatt)\n",
        "dense2 = tf.keras.layers.Dense(units=84, activation=activation,name='dense2')(dense1)\n",
        "dense3 = tf.keras.layers.Dense(units=10,name='dense3')(dense2)\n",
        "\n",
        "logits = dense3\n",
        "model = tf.keras.Model(inputs=inputs,outputs=logits)\n",
        "\n",
        "## data\n",
        "\n",
        "(x_train,y_train),_ = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train / 255\n",
        "x_train = x_train - x_train.mean(0)\n",
        "y_train = y_train.astype('int32').reshape(-1,1)\n",
        "x_train = x_train.astype('float32')\n",
        "\n",
        "x,y = x_train[:64],y_train[:64]\n",
        "\n",
        "## estimate hessian at init\n",
        "\n",
        "layers = ['conv1','conv2','dense1','dense2','dense3']\n",
        "diffs = {layer:[] for layer in layers}\n",
        "\n",
        "for layer in layers:\n",
        "  \n",
        "  for _ in range(100):\n",
        "  \n",
        "    reset_kernels(model)\n",
        "    \n",
        "    var = model.get_layer(layer).kernel\n",
        "    #var.assign(layer.kernel_initializer(var.shape))\n",
        "    \n",
        "    error = happrox_error(model,var,x,y).numpy()\n",
        "\n",
        "    diffs[layer].append(error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KTKpMbOMNLb"
      },
      "source": [
        "fig = plt.figure(constrained_layout=False,figsize=(10,10))\n",
        "gs = fig.add_gridspec(nrows=2, ncols=6)#, left=0.05, right=0.48, wspace=0.05)\n",
        "ax1 = fig.add_subplot(gs[0,0:2])\n",
        "ax2 = fig.add_subplot(gs[0,2:4])\n",
        "ax3 = fig.add_subplot(gs[0,4:6])\n",
        "ax4 = fig.add_subplot(gs[1,0:3])\n",
        "ax5 = fig.add_subplot(gs[1,3:6])\n",
        "axs = [ax1,ax2,ax3,ax4,ax5]\n",
        "names = ['dense1','dense2','dense3','conv1','conv2']\n",
        "\n",
        "for ax,name in zip(axs,names):\n",
        "  sn.distplot(diffs[name],ax=ax)\n",
        "  ax.legend([name])\n",
        "\n",
        "fig.add_subplot(111, frame_on=False)\n",
        "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
        "plt.xlabel('relative hessian approximation error at init')\n",
        "plt.title('LeNet-5 on CIFAR',fontweight=\"bold\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAdci7V-nJeu"
      },
      "source": [
        "## Hessian-driven initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFW15feInOQh"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "## prepare data\n",
        "(x_train,y_train),_ = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_train = x_train - x_train.mean(0)\n",
        "y_train = y_train.astype('int32')\n",
        "x_train = x_train.astype('float32')\n",
        "data = tf.data.Dataset.from_tensor_slices((x_train,y_train.reshape(-1,1))).batch(64).repeat(2)\n",
        "\n",
        "hforms = []\n",
        "losses = []\n",
        "\n",
        "## build model; try different inits\n",
        "\n",
        "setups = [(tf.keras.activations.relu,tf.keras.initializers.truncated_normal(stddev=0.3),'relu + *hessian driven stddev*'),\n",
        "          (tf.keras.activations.relu,tf.keras.initializers.he_normal(),'relu + He normal'),\n",
        "          (tf.keras.activations.tanh,tf.keras.initializers.glorot_uniform(),'tanh + Glorot'),\n",
        "          (tf.keras.activations.tanh,tf.keras.initializers.orthogonal(),'tanh + Orthogonal'),\n",
        "          (tf.keras.activations.tanh,tf.keras.initializers.orthogonal(),'relu + Orthogonal')\n",
        "          ]\n",
        "\n",
        "#activations = {'sigmoid':tf.keras.activations.sigmoid,'relu':tf.keras.activations.relu,'tanh':tf.keras.activations.tanh}\n",
        "\n",
        "for setup in setups:\n",
        "\n",
        "  activation,init,name = setup\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(28,28,))\n",
        "  flatt = tf.keras.layers.Flatten()(inputs)\n",
        "  if name == 'relu + *hessian driven stddev*':\n",
        "    init1 = 'glorot_uniform'\n",
        "  else:\n",
        "    init1 = init\n",
        "  init2,init3 = init,init\n",
        "  dense1 = tf.keras.layers.Dense(128,name='dense1',activation=activation,kernel_initializer=init1)(flatt)\n",
        "  dense2 = tf.keras.layers.Dense(84,name='dense2',activation=activation,kernel_initializer=init2)(dense1)\n",
        "  dense3 = tf.keras.layers.Dense(10,name='dense3',activation='linear',kernel_initializer=init3)(dense2)\n",
        "  logits = dense3\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "  ## prepare optimizer\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "  vars = model.trainable_variables\n",
        "\n",
        "  layers = ['dense1','dense2','dense3']\n",
        "\n",
        "  ## estimate hessian at init\n",
        "  for layer in layers:\n",
        "    \n",
        "    for _ in range(100):\n",
        "    \n",
        "      reset_kernels(model)\n",
        "      \n",
        "      x,y = x_train[:100],y_train[:100]\n",
        "      var = model.get_layer(layer).kernel\n",
        "      vec = grad(model,var,x,y)\n",
        "      \n",
        "      h = hform(model,var,vec,x,y)\n",
        "      h = h/tf.linalg.norm(vec)**2\n",
        "\n",
        "      hforms.append( (name,layer,h.numpy()) )\n",
        "\n",
        "  ## train\n",
        "\n",
        "  for i,(x,y) in enumerate(data):\n",
        "    \n",
        "    grads = grad(model,vars,x,y)\n",
        "    optimizer.apply_gradients(zip(grads,vars))\n",
        "\n",
        "    if i%100 == 0:\n",
        "      loss = logp(model(x_train),y_train)\n",
        "      losses.append( (name,i,loss.numpy()) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pTiyBU0n9fv"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "hforms = pd.DataFrame(hforms)\n",
        "hforms.columns = ['setup','layer','init hessian']\n",
        "losses = pd.DataFrame(losses)\n",
        "losses.columns = ['setup','step','loss']\n",
        "\n",
        "sn.set_style(\"darkgrid\")\n",
        "\n",
        "fig,axs = plt.subplots(len(setups),2,figsize=(10,10))\n",
        "\n",
        "for (i,setup) in enumerate(setups):\n",
        "  _,_,s = setup\n",
        "  title = s\n",
        "  ax = axs[i][0]\n",
        "  for l in layers:\n",
        "    mask = (hforms['setup'] == s) & (hforms['layer'] == l)\n",
        "    sn.kdeplot(x=hforms[mask]['init hessian'],ax=ax,log_scale=True,label=l)\n",
        "    ax.set_title(title,fontweight='bold')\n",
        "    ax.legend()\n",
        "\n",
        "  mask = losses['setup'] == s\n",
        "  ax = axs[i][1]\n",
        "  sn.lineplot(losses[mask]['step'],-losses[mask]['loss'],ax=ax)\n",
        "  ax.set_title(title,fontweight='bold')\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2BEMAv4rKGT"
      },
      "source": [
        "step_max = losses['step'].max()\n",
        "-losses.groupby('setup')[['loss']].max()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}